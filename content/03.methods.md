## Methods

### Algorithm

Our method MapperGPT takes as input two ontologies *O1* and *O2* and a set of candidate mappings *M*.
These mappings are assumed to be have been generated from an existing high-recall method such as LOOM.

```
M' = {}
For m in M:
  prompt = GeneratePrompt(m.a, m.b, O1, O2)
  response = CompletePrompt(prompt, model)
  m' = Parse(response)
  add m' to M'
return M'
```

### Prompt generation

The method `GeneratePrompt` generates a prompt according to the following template:
    
{% raw %}
```
What is the relationship between the two specified concepts?

Give your answer in the form:

category: <one of: EXACT_MATCH, BROADER_THAN, NARROWER_THAN, RELATED_TO, DIFFERENT>
confidence: <one of: LOW, HIGH, MEDIUM>
similarities: <semicolon-separated list of similarities>
differences: <semicolon-separated list of differences>

Make use of all provided information, including the concept names, definitions, and relationships.

Examples:

{{ examples }}

Here are the two concepts:

{{ Describe(conceptA) }}
{{ Describe(conceptB) }}
```
{% endraw %}

The use of examples makes this a few-shot learning approach.

Examples are provided in-context in the following form:

```
[Concept A]
id: FOO:125
name: wing
def: part of a bird that is flapped to enable flight
is_a: Limb
relationship: part_of Bird
relationship: has_part Feather

[Concept B]
id: BAR:458
name: wing
relationship: part_of Aeroplane

category: DIFFERENT
confidence: HIGH
similarities: function
differences: A is an anatomical part; B is a part of a vehicle
```

For each candidate mapping between concepts A and B, we generate a description of each concept,
incorporating key elements: name, synonyms, definition, relationships. 

The `Describe` function will generate a textual description of an ontology or database concept, showing the
following properties:

- name
- synonyms
- definition
- parents (superclasses)
- other relationships

### Prompt Completion

The prompt is then passed to a GPT model, which generates a response.
In principle the method should work with any instruction-based model, either
local or remotely accessed via an API. In practice we have only
evaluated this against the OpenAI API and the two leading instruction-based
models, `gpt-3.5-turbo` and `gpt-4`.

### Response Parsing

The response is parsed to retrieve
the key data model elements: category, confidence, similarities, differences.

The result object can be exported to SSSOM format.

### Example

As an example, two concepts from the fruitfly and zebra fish anatomy ontologies are candidate matches
due to sharing a lexical element (the "PC" abbreviation). This is a false positive match in reality,
as the concept are entirely different.

The two concept descriptions are generated from respective ontologies as follows:

```yaml
[Concept A]
id: FBbt:00001906
name: embryonic/larval Malpighian tubule Type I cell
def: Type I cell of the embryonic/larval Malpighian tubules.
synonyms:  PC ;  embryonic/larval Malpighian tubule Type I cell ;  larval Malpighian tubule Type I cell ;  larval Malpighian tubule principal cell ; 
is_a:  embryonic/larval specialized Malpighian tubule cell ;  Malpighian tubule Type I cell ; 

[Concept B]
id: ZFA:0000320
name: caudal commissure
def: Diencephalic tract which is located in the vicinity of the dorsal diencephalon and mesencephalon and connects the pretectal nuclei. From Neuroanatomy of the Zebrafish Brain.
synonyms:  PC ;  caudal commissure ;  posterior commissure ; 
is_a:  diencephalic white matter ; 
relationship: part of synencephalon
relationship: start stage unknown
relationship: end stage adult
```

The payload for this using gpt-3.5-turbo is:

```yaml
category: DIFFERENT
confidence: HIGH
similarities: NONE
differences: A is a type of cell in the embryonic/larval Malpighian tubules; B is a diencephalic tract in the zebrafish brain.
subject: FBbt:00001906
object: ZFA:0000320
```

This is then parsed to a YAML object:

```yaml
predicate: DIFFERENT
confidence: HIGH
similarities:
  - NONE
differences:
  - A is a type of cell in the embryonic/larval Malpighian tubules
  - B is a diencephalic tract in the zebrafish brain.
```

The consumer may typically only make use of the *predicate* slot, but the list of
similarities and differences may prove informative.

### Implementation

We use the OAK library to connect to a variety of ontologies in OBO and Bioportal.
The overall framework is implemented in OntoGPT.

The input is an SSSOM file. The output is SSSOM with predicate_id filled with predicted value:

```bash
ontogpt categorize-mappings --model gpt-4 -i foo.sssom.tsv -o bar.sssom.tsv
```

### Generation of test sets

To evaluate the method, we created a collection of test sets from biological domains.
We chose to devise new test sets as we wanted to base these on up-to-date, precise,
validated mappings derived from ontologies such as Mondo, CL, and Uberon.

To generate anatomy test sets, we generated pairwise mappings between species-specific anatomy ontologies,
using the Uberon and CL mappings as the gold standard. i.e. if a pair of concepts are transitively linked
via Uberon or CL, then they are considered a match. We used the same method for developmental stages.

We also generated a renal disease test set by taking all heritable renal diseases from Mondo, all
renal diseases from NCIT, and generating a test set based on validated curated mappings between
Mondo and NCIT.

TODO: more test sets

TODO: table showing sizes

### Tool evaluation

We evaluate MapperGPT with two models: gpt-3.5-turbo and gpt-4. MapperGPT is capable of providing
refined predicates from SKOS but for this task we only take exactMatch as a predicted mapping,
and discard all others.

We also evaluated against the OAK lexmatch tool, as a high-recall baseline. Although lexmatch
allows for customizable rules, we ran this without any prior knowledge of the domains, and
considered any lexical match to be a predicted match.

We selected LogMap, which is one of the top-performing mappers in the OAEI. 
We convert LogMap results to SSSOM [doi@10.1093/database/baac035]. (Harshad to write)

LogMap produces a score with each mapping, so we scanned all scores to determine the optimal score threshold
in terms of accuracy (F1) (note this gives LogMap an advantage over our method, which does not produce a score).

